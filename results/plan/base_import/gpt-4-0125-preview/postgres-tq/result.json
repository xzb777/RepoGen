{
    "implementation": [
        {
            "key_id": 0,
            "fqn": "postgrestq/task_queue.py/TaskQueue/__init__",
            "imports": [
                "import logging",
                "from typing import Optional, Callable",
                "from uuid import UUID",
                "import psycopg",
                "from psycopg import sql"
            ],
            "code": "def __init__(self, dsn: str, queue_name: str, table_name: str = 'task_queue', reset: bool = False, create_table: bool = False, ttl_zero_callback: Optional[Callable[[UUID, Optional[str]], None]] = None):\n    self.dsn = dsn\n    self.queue_name = queue_name\n    self.table_name = table_name\n    self.reset = reset\n    self.create_table = create_table\n    self.ttl_zero_callback = ttl_zero_callback\n    self.logger = logging.getLogger(__name__)\n    if self.create_table:\n        self._create_queue_table()\n    if self.reset:\n        self._reset()"
        },
        {
            "key_id": 2,
            "fqn": "postgrestq/task_queue.py/TaskQueue/connect",
            "imports": [
                "import psycopg"
            ],
            "code": "def connect(self) -> None:\n    self.conn = psycopg.connect(self.dsn)\n    self.cur = self.conn.cursor()"
        },
        {
            "key_id": 4,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_create_queue_table",
            "imports": [
                "from psycopg import sql"
            ],
            "code": "def _create_queue_table(self) -> None:\n    create_table_query = sql.SQL('''CREATE TABLE IF NOT EXISTS {table} (\n        id UUID PRIMARY KEY,\n        queue_name TEXT NOT NULL,\n        task JSONB NOT NULL,\n        ttl INT NOT NULL,\n        created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n        processing BOOLEAN NOT NULL DEFAULT false,\n        lease_timeout FLOAT,\n        deadline TIMESTAMP,\n        completed_at TIMESTAMP\n    )''').format(table=sql.Identifier(self.table_name))\n    self.cur.execute(create_table_query)\n    self.conn.commit()"
        },
        {
            "key_id": 10,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_serialize",
            "imports": [
                "import json"
            ],
            "code": "def _serialize(self, task: Any) -> str:\n    return json.dumps(task)"
        },
        {
            "key_id": 11,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_deserialize",
            "imports": [
                "import json"
            ],
            "code": "def _deserialize(self, blob: str) -> Any:\n    return json.loads(blob)"
        },
        {
            "key_id": 1,
            "fqn": "postgrestq/task_queue.py/TaskQueue/connect",
            "imports": [
                "import logging",
                "from psycopg import connect"
            ],
            "code": "def connect(self) -> None:\n    try:\n        self.connection = connect(self.dsn)\n        self.cursor = self.connection.cursor()\n        logging.info('Successfully connected to the database.')\n    except Exception as e:\n        logging.error(f'Failed to connect to the database: {e}')"
        },
        {
            "key_id": 2,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_create_queue_table",
            "imports": [
                "import logging",
                "from psycopg import sql"
            ],
            "code": "def _create_queue_table(self) -> None:\n    create_table_query = sql.SQL(\"\"\"CREATE TABLE IF NOT EXISTS {table} (\n        id UUID PRIMARY KEY,\n        queue_name TEXT NOT NULL,\n        task JSONB NOT NULL,\n        ttl INT NOT NULL,\n        created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n        processing BOOLEAN NOT NULL DEFAULT false,\n        lease_timeout FLOAT,\n        deadline TIMESTAMP,\n        completed_at TIMESTAMP\n    )\"\"\").format(table=sql.Identifier(self.table_name))\n    try:\n        self.cursor.execute(create_table_query)\n        self.connection.commit()\n        logging.info('Table created successfully.')\n    except Exception as e:\n        logging.error(f'Failed to create table: {e}')"
        },
        {
            "key_id": 10,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_serialize",
            "imports": [
                "import json"
            ],
            "code": "def _serialize(self, task: Any) -> str:\n    try:\n        return json.dumps(task)\n    except TypeError as e:\n        raise ValueError(f'Unable to serialize task: {e}') from e"
        },
        {
            "key_id": 13,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_reset",
            "imports": [
                "import logging",
                "from psycopg import sql"
            ],
            "code": "def _reset(self) -> None:\n    reset_query = sql.SQL('TRUNCATE TABLE {table}').format(table=sql.Identifier(self.table_name))\n    try:\n        self.cursor.execute(reset_query)\n        self.connection.commit()\n        logging.info('Table reset successfully.')\n    except Exception as e:\n        logging.error(f'Failed to reset table: {e}')"
        },
        {
            "key_id": 14,
            "fqn": "postgrestq/task_queue.py/TaskQueue/prune_completed_tasks",
            "imports": [
                "import logging",
                "from psycopg import sql"
            ],
            "code": "def prune_completed_tasks(self, before: int) -> None:\n    prune_query = sql.SQL(\"\"\"DELETE FROM {table} WHERE completed_at IS NOT NULL AND EXTRACT(EPOCH FROM (NOW() - completed_at)) > %s\"\"\").format(table=sql.Identifier(self.table_name))\n    try:\n        self.cursor.execute(prune_query, (before,))\n        self.connection.commit()\n        logging.info(f'Pruned tasks older than {before} seconds.')\n    except Exception as e:\n        logging.error(f'Failed to prune tasks: {e}')"
        },
        {
            "key_id": 11,
            "fqn": "postgrestq/task_queue.py/TaskQueue/_deserialize",
            "imports": [
                "import json"
            ],
            "code": "def _deserialize(self, blob: str) -> Any:\n    try:\n        return json.loads(blob)\n    except json.JSONDecodeError as e:\n        logger.error(f'Error decoding JSON from blob: {blob}, error: {e}')\n        raise ValueError('Invalid JSON format') from e"
        },
        {
            "key_id": 3,
            "fqn": "postgrestq/task_queue.py/TaskQueue/__len__",
            "imports": [
                "import psycopg.connect"
            ],
            "code": "def __len__(self) -> int:\n    with psycopg.connect(self.dsn) as conn:\n        with conn.cursor() as cur:\n            cur.execute('SELECT COUNT(*) FROM ' + self.table_name + ' WHERE completed_at IS NULL')\n            (count,) = cur.fetchone()\n            return count"
        },
        {
            "key_id": 4,
            "fqn": "postgrestq/task_queue.py/TaskQueue/add",
            "imports": [
                "import json",
                "import psycopg.connect",
                "from uuid import uuid4"
            ],
            "code": "def add(self, task: Dict[str, Any], lease_timeout: float, ttl: int = 3) -> str:\n    task_id = uuid4()\n    serialized_task = json.dumps(task)\n    with psycopg.connect(self.dsn) as conn:\n        with conn.cursor() as cur:\n            cur.execute('INSERT INTO ' + self.table_name + ' (id, queue_name, task, ttl, lease_timeout, processing) VALUES (%s, %s, %s, %s, %s, %s)', (str(task_id), self.queue_name, serialized_task, ttl, lease_timeout, False))\n            conn.commit()\n    return str(task_id)"
        },
        {
            "key_id": 5,
            "fqn": "postgrestq/task_queue.py/TaskQueue/get",
            "imports": [
                "import psycopg.connect",
                "import json",
                "from uuid import UUID"
            ],
            "code": "def get(self) -> Tuple[Optional[Dict[str, Any]], Optional[UUID]]:\n    with psycopg.connect(self.dsn) as conn:\n        with conn.cursor() as cur:\n            cur.execute('UPDATE ' + self.table_name + ' SET processing = TRUE, deadline = current_timestamp + (lease_timeout || '' seconds'')::interval WHERE id = (SELECT id FROM ' + self.table_name + ' WHERE completed_at IS NULL AND processing = FALSE AND queue_name = %s AND ttl > 0 ORDER BY created_at FOR UPDATE SKIP LOCKED LIMIT 1) RETURNING id, task', (self.queue_name,))\n            result = cur.fetchone()\n            if result:\n                task_id, task_blob = result\n                return json.loads(task_blob), UUID(task_id)\n            else:\n                return None, None"
        },
        {
            "key_id": 6,
            "fqn": "postgrestq/task_queue.py/TaskQueue/complete",
            "imports": [
                "import psycopg.connect"
            ],
            "code": "def complete(self, task_id: Optional[UUID]) -> None:\n    if task_id is None:\n        logger.error('Task ID is None, cannot mark as complete')\n        return\n    with psycopg.connect(self.dsn) as conn:\n        with conn.cursor() as cur:\n            cur.execute('UPDATE ' + self.table_name + ' SET completed_at = current_timestamp WHERE id = %s', (str(task_id),))\n            conn.commit()"
        },
        {
            "key_id": 7,
            "fqn": "postgrestq/task_queue.py/TaskQueue/is_empty",
            "imports": [
                "import logging",
                "from psycopg import connect",
                "from psycopg import sql"
            ],
            "code": "def is_empty(self) -> bool:\n    logger = logging.getLogger(__name__)\n    try:\n        with connect(self.dsn) as conn:\n            with conn.cursor() as cur:\n                cur.execute(sql.SQL(\"SELECT COUNT(*) FROM {} WHERE completed_at IS NULL AND processing = FALSE\").format(sql.Identifier(self.table_name)))\n                count = cur.fetchone()[0]\n                return count == 0\n    except Exception as e:\n        logger.error(f'Error checking if queue is empty: {e}')\n        return False"
        },
        {
            "key_id": 8,
            "fqn": "postgrestq/task_queue.py/TaskQueue/check_expired_leases",
            "imports": [
                "import logging",
                "from psycopg import connect",
                "from psycopg import sql",
                "from datetime import datetime, timedelta"
            ],
            "code": "def check_expired_leases(self) -> None:\n    logger = logging.getLogger(__name__)\n    try:\n        with connect(self.dsn) as conn:\n            with conn.cursor() as cur:\n                cur.execute(sql.SQL(\"UPDATE {} SET processing = FALSE, completed_at = NULL WHERE lease_timeout < %s AND processing = TRUE AND completed_at IS NULL\").format(sql.Identifier(self.table_name)), [datetime.now() - timedelta(seconds=self.lease_timeout)])\n    except Exception as e:\n        logger.error(f'Error checking expired leases: {e}')"
        },
        {
            "key_id": 9,
            "fqn": "postgrestq/task_queue.py/TaskQueue/get_updated_expired_task",
            "imports": [
                "import logging",
                "from psycopg import connect",
                "from psycopg import sql"
            ],
            "code": "def get_updated_expired_task(self, task_id: UUID) -> Tuple[Optional[str], Optional[int]]:\n    logger = logging.getLogger(__name__)\n    try:\n        with connect(self.dsn) as conn:\n            with conn.cursor() as cur:\n                cur.execute(sql.SQL(\"SELECT task, ttl FROM {} WHERE id = %s AND processing = TRUE AND completed_at IS NULL\").format(sql.Identifier(self.table_name)), [task_id])\n                result = cur.fetchone()\n                if result:\n                    return result\n                else:\n                    return None, None\n    except Exception as e:\n        logger.error(f'Error getting updated expired task: {e}')\n        return None, None"
        },
        {
            "key_id": 12,
            "fqn": "postgrestq/task_queue.py/TaskQueue/reschedule",
            "imports": [
                "import logging",
                "from psycopg import connect",
                "from psycopg import sql",
                "from datetime import datetime"
            ],
            "code": "def reschedule(self, task_id: Optional[UUID]) -> None:\n    logger = logging.getLogger(__name__)\n    if task_id is None:\n        logger.error('Task ID is None, cannot reschedule.')\n        return\n    try:\n        with connect(self.dsn) as conn:\n            with conn.cursor() as cur:\n                cur.execute(sql.SQL(\"UPDATE {} SET processing = FALSE, completed_at = NULL, deadline = %s WHERE id = %s AND processing = TRUE\").format(sql.Identifier(self.table_name)), [datetime.now(), task_id])\n    except Exception as e:\n        logger.error(f'Error rescheduling task {task_id}: {e}')"
        },
        {
            "key_id": 15,
            "fqn": "postgrestq/task_queue.py/TaskQueue/__iter__",
            "imports": [
                "import logging",
                "from psycopg import connect",
                "from psycopg import sql",
                "from uuid import UUID",
                "from typing import Tuple, Optional, Dict, Any, Iterator"
            ],
            "code": "def __iter__(self) -> Iterator[Tuple[Optional[Dict[str, Any]], Optional[UUID]]]:\n    logger = logging.getLogger(__name__)\n    try:\n        with connect(self.dsn) as conn:\n            with conn.cursor() as cur:\n                cur.execute(sql.SQL(\"SELECT id, task FROM {} WHERE processing = FALSE AND completed_at IS NULL ORDER BY created_at ASC\").format(sql.Identifier(self.table_name)))\n                for record in cur.fetchall():\n                    yield self._deserialize(record[1]), UUID(record[0])\n    except Exception as e:\n        logger.error(f'Error iterating over tasks: {e}')\n        yield None, None"
        }
    ]
}